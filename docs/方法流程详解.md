# Episodic Steering Memory (ESM) 方法流程详解（steer_memory_4.5.3）

本文档面向本仓库（`steer_memory_4.5.3`）的**实现级**复现：从数据与 prompt、离线三阶段（mine/select/memory），到在线 ESM 控制器（检索→探测→注入→分段生成），以及最终评测与产物落盘。公式以论文草稿 `idea4.5.2.tex` 为主，并对齐当前代码路径（尤其是 v4.5.3 的记忆条目格式与打分）。

---

## 0. 全流程概览（对应 `run.py` 子命令）

一次完整实验按以下顺序组织（每步都写到 `outputs/<run_name>/<run_id>/`）：

1. **Stage I / mine**：对训练集做对比 rollouts，挖掘候选记忆条目（`esm/offline/stage1.py:mine_candidates`）。
2. **Stage II / select**：从候选中选出一个高质量且多样的工具库（`esm/offline/stage2.py:select_library`）。
3. **Stage III / memory**：把工具库索引成可在线检索的 episodic memory（`esm/offline/stage3.py:build_memory`）。
4. **eval（在线）**：
   - `greedy`：普通贪心/temperature=0 解码基线（`esm/online/greedy.py:run_greedy_dataset`）。
   - `esm`：分段 + 检索 + 探测 + 稀疏注入（`esm/online/esm.py:run_esm_dataset`）。

典型运行：

```bash
python run.py --config configs/debug.yaml mine
python run.py --config configs/debug.yaml select
python run.py --config configs/debug.yaml memory
python run.py --config configs/debug.yaml eval
```

### 0.1 运行环境与依赖（EasySteer/vLLM）

- `mine` 与 `eval` 需要 GPU；`select/memory` 主要是读写向量文件（CPU 即可）。
- 本仓库依赖带 **steer vector** 与 **hidden-states capture** 的 vLLM 版本，并通过 `easysteer.hidden_states` 提供的 RPC 封装抓取隐藏态；通常建议在 conda 环境 `easysteer` 中运行。
- 建议设置：`TOKENIZERS_PARALLELISM=false`，并用 `CUDA_VISIBLE_DEVICES=...` 显式选择 GPU。

### 0.2 Batch size 与“并行度/slots”是两件事

本仓库里会同时出现三类“批量”概念（后文各节会展开）：

- **数据批（examples batch）**：`offline_mine.batch_size_examples` / `online.batch_size_examples`，控制一次喂给 `llm.generate()` 多少个 *dataset example*。
- **采样扩增（rollouts）**：Stage I 用 `SamplingParams.n=offline_mine.K` 为每个 example 生成 \(K\) 条 completion；一次调用内部实际处理的序列数近似为 `batch_size_examples * K`。
- **vLLM 并行上限 / steer slots**：`model.max_num_seqs` 影响 vLLM 的调度并行与显存占用；同时我们也用它做 hidden-states capture 的 chunk 大小，并用 vLLM 的 `max_steer_vectors`（在线初始化通常设为 `max(8, model.max_num_seqs)`）对“同一批次可用的 steer vector 数”做硬约束与分块。

---

## 1. 记号与对象（Notation）

### 1.1 任务与数据

- 任务/数据集：`task.dataset`（例如 `gsm8k`、`math500`、`arc-c` 等），统一样本结构为
  \[
  x := (\text{question}, \text{answer}, \text{meta})
  \]
  见 `esm/data/tasks.py:TaskExample` 与 `esm/data/loaders.py:load_task_dataset`。

### 1.2 模型与激活（Activation）

- 冻结的自回归语言模型 \(p_\theta\)（参数 \(\theta\) 固定）。
- 生成时刻 \(t\)（即“序列里的第 \(t\) 个 token 位置/时间步”，而不是“第 \(t\) 个 step”）在第 \(\ell\) 层的 residual stream（或等价隐藏态）记为：
  \[
  h^{(\ell)}_t \in \mathbb{R}^{d}
  \]
  这里的 \(t\) 是**token 下标**：Transformer 的激活天然是“每层、每个 token 位置各一个向量”，所以无论你用什么规则划分 step/segment，最终都要落到某个 token 位置 \(t\) 才能取到 \(h^{(\ell)}_t\) 并做注入。

  对于本仓库的 delimiter 分段（默认 `"\n\n"`），第 \(m\) 个控制点对应到一个具体的 token 位置 \(t_m\)（“前缀到第 \(m\) 个 delimiter 为止”的最后一个 token）。实现上我们不显式计算 \(t_m\)，而是把该前缀整段送进 vLLM 取全序列 hidden states，然后直接取最后一个位置 `[-1]`，即
  \[
  h^{(\ell)}_{t_m} \equiv h^{(\ell)}_{\text{last}}(\text{prefix}_m)
  \]
  具体实现使用 `easysteer.hidden_states.get_all_hidden_states_generate`（代码里简称 `hs.get_all_hidden_states_generate`；见 `esm/offline/stage1.py` 与 `esm/online/esm.py`）：

  - 调用形式：`hs.get_all_hidden_states_generate(llm, [prefix], max_tokens=1, split_by_samples=True)`。
  - 返回结构：`hidden_states[sample_idx][layer_idx]`，每层是形状 `(seq_len, hidden_size)` 的张量。
  - 关键点：当 `max_tokens=1` 时，EasySteer 会用“只生成 1 个 token 来触发 forward”的方式抓取 **prompt（即 prefix）部分**的 hidden states，因此此处 `seq_len ≈ len(prompt_token_ids)`，而 `states[layer][-1]` 就对应前缀最后一个 token 的 \(h^{(\ell)}_{t_m}\)。

### 1.3 控制点（Control Point）与分段（Segmentation）

- 设 delimiter 为一段字符串 \(D\)（默认空行 `"\n\n"`；`control_points.segment_delimiter`）。
- 将一次回答按 delimiter 切成最多 \(M\) 段（`control_points.M`）：
  \[
  y = [y^{(1)}, y^{(2)}, \dots, y^{(M)}]
  \]
  其中 \(y^{(m)}\) 是第 \(m-1\) 个 delimiter 到第 \(m\) 个 delimiter 之间生成的 token 序列（代码实现以“首次出现 delimiter”作为段结束，见 `esm/offline/stage1.py:_delimiter_end_offsets` 与 `esm/online/esm.py` 中 `stop=delimiter`）。

**离线/在线的 control_point_m 定义要严格区分：**

- **离线 Stage I**：对第 \(m\) 个 delimiter 位置（即“生成完第 \(m\) 段以后”）的前缀状态建条目，记为 `control_point_m = m`。
- **在线 ESM**：在生成第 \(m\) 段之前决策，因此使用
  \[
  \texttt{mem\_m} = m - 1
  \]
  去检索离线中 “生成完第 \(m-1\) 段以后” 的条目（代码：`esm/online/esm.py` 中 `mem_m = int(m) - 1`）。

---

## 2. Steering 工具的注入算子（与 EasySteer 对齐）

一个 steering 工具（tool）由三元组构成：
\[
b = (v_b, \ell_b, \alpha_b)
\]
其中 \(v_b\in\mathbb{R}^d\) 是注入向量，\(\ell_b\) 是注入层，\(\alpha_b\) 是注入强度（scale）。

论文（`idea4.5.2.tex`）给出的基础注入算子为：
\[
\mathcal{S}_{\ell_b,t}\!\left(h_t^{(\ell_b)}; v_b,\alpha_b\right) := h_t^{(\ell_b)} + \alpha_b\, v_b
\]

本仓库实现使用 EasySteer 的 `SteerVectorRequest`（见 `esm/online/esm.py` 中 `_new_steer_request`）：

- **注入位置**：prefill 阶段的指定 token 位置（默认 `prefill_trigger_positions=[-1]` 即“当前前缀最后一个 token”）。
- **注入时机**：默认不在 generation phase 注入（`generate_trigger_tokens=None`），保证“只在控制点稀疏注入”。
- **算法**：`algorithm="direct"`，且 `normalize=False`，即**不对向量做归一化**（保留 \(\|v\|\) 的“内禀 steering 强度”）。

> 注：EasySteer 的底层实现细节以其代码为准；本仓库的公式化描述与论文一致：在指定层、指定控制点 token 上做向量加法。

---

## 3. Stage I（mine）：对比 rollouts 挖掘候选记忆条目

代码入口：`esm/offline/stage1.py:mine_candidates`  
产物目录：`outputs/<run>/mine/`

### 3.1 生成对比 rollouts

对每个训练样本 \(x\in\mathcal{D}\)，用同一个 prompt 生成 \(K\) 条 rollout（`offline_mine.K`），记为：
\[
\{y^{(k)}\}_{k=1}^{K}
\]

- prompt 由 `prompt.template` 决定（`esm/data/prompts.py:render_user_prompt`），并通过 tokenizer chat template 拼成最终字符串（示例实现见 `esm/online/greedy.py` / `esm/online/esm.py`）。
- rollout 的最大生成 token 数为 `offline_mine.max_new_tokens`（与在线评测的 `decode.max_new_tokens` 可不同）。

#### 3.1.1 Stage I 的 batch size（`offline_mine.batch_size_examples`）

- `mine` 以“数据集 example”为单位做 batching：一次 `llm.generate(batch_prompts, SamplingParams(n=K))` 处理 `batch_size_examples` 个 prompt（见 `esm/offline/stage1.py` 的 batch loop）。
- 因为 `n=K` 会为每个 prompt 生成 \(K\) 条 completion，一次调用内部的有效并行规模近似为 `offline_mine.batch_size_examples * offline_mine.K`；显存不足时通常先降 `batch_size_examples` 或 `K`，再调 `model.max_num_seqs / model.gpu_memory_utilization`。
- 隐藏态抓取（第 3.5 节）是另一种批处理：我们把需要抓取的“正/负前缀文本”收集到 `all_texts`，并用 `hs_batch_size = model.max_num_seqs` 分块调用 `hs.get_all_hidden_states_generate(..., max_tokens=1)`。

### 3.2 终止奖励与长度正则（Length-regularized reward）

令 \(R(x,y)\in\{0,1\}\) 为最终答案是否正确（由 extractor+metric 判定，见 `esm/eval/extractors.py` 与 `esm/eval/metrics.py`）。定义长度正则化奖励（论文 Eq. (online_obj)，代码 `esm/offline/stage1.py:_length_regularized_reward`）：
\[
R_\eta(x,y) := R(x,y) - \eta \cdot \frac{|y|}{T_{\max}}
\]

在 Stage I 中具体是：
- \(\eta=\eta_0\)（`offline_mine.eta0`）
- \(T_{\max}=\) `offline_mine.max_new_tokens`
- \(|y|\) 为 rollout 实际生成 token 数（`len(token_ids)`）

于是每条 rollout 的标量 reward 为：
\[
r^{(k)} := R_{\eta_0}\!\left(x,y^{(k)}\right)
\]

### 3.3 控制点前缀的对齐（delimiter-based alignment）

对每条 rollout 文本 \(y^{(k)}\)，找出 delimiter \(D\) 的所有出现位置，并取第 \(m\) 次出现的**结束字符偏移**作为段终点（代码 `esm/offline/stage1.py:_delimiter_end_offsets`）。于是对控制点 \(m\in\{1,\dots,M\}\)，定义“到第 \(m\) 个 delimiter 为止的前缀文本”：
\[
\text{prefix}^{(k)}_m := \text{prompt} \;+\; y^{(k)}[0:\text{end\_offset}^{(k)}_m]
\]
只有当 rollout 至少出现 \(m\) 次 delimiter 时，这个控制点才有效。

实现注记：

- Stage I rollout 生成本身**不使用** `stop=delimiter`，而是在生成完以后再用字符串查找 delimiter；因此若模型几乎不输出 delimiter，会导致很多 control point 无效、候选稀疏（通常需要调整 prompt 模板或 delimiter）。
- prefix 截断使用的是 detokenized 文本的“字符偏移”切片；后续 hidden-states capture 会对 `prompt + prefix_slice` 重新 tokenize（实践中通常稳定，但这不是严格的 token-level 对齐）。

### 3.4 正/负集合的构造（Pos/Neg sets）

对每个样本 \(x\) 和控制点 \(m\)，从“在该控制点有效”的 rollouts 中构造正集合 \(\mathcal{P}(x,m)\) 与负集合 \(\mathcal{N}(x,m)\)：

- **默认（推荐）**：要求出现“正确 vs 错误”的对比（`offline_mine.require_correct_and_incorrect=true`）。
  - \(\mathcal{P}\)：正确 rollouts（或 gold 作为兜底，见下）
  - \(\mathcal{N}\)：错误 rollouts
  - `offline_mine.min_correct_rollouts / min_incorrect_rollouts` 控制最小数量；不足则跳过该控制点（避免挖到“短错 vs 长错”的噪声差分）。
  - `offline_mine.K_pos / K_neg` 决定各取多少条：正样本取 reward 最大的 top-\(K_{pos}\)，负样本取 reward 最小的 bottom-\(K_{neg}\)（实现见 `esm/offline/stage1.py`；在 \(R\in\{0,1\}\) 且 \(\eta_0>0\) 时，错误 rollout 的 reward 越小通常意味着生成更长、更“过度推理”的负例）。

- **gold 兜底正样本（pos_source）**：当正确 rollouts 很少时可用 `offline_mine.pos_source ∈ {rollout,gold,rollout_or_gold}`。
  - `gold` / `rollout_or_gold` 会尝试把数据集 gold 解答当作“正文本”，从而仍可形成对比（代码中对 AIME 还会用 filler 反复补齐段数，使 gold 文本覆盖 \(M\) 个 delimiter）。

### 3.5 对比隐藏态与 raw delta（key/value 的构造）

对每个控制点 \(m\) 和候选层 \(\ell\in\mathcal{L}\)（`offline_mine.candidate_layers`；为空时自动取“顶层 1/3”），取每个前缀 \(\text{prefix}^{(k)}_m\) 的最后一 token 隐藏态：
\[
h_{m}^{(\ell)}(x, y^{(k)}) \;:=\; h^{(\ell)}_{t_m}\!\left(\text{prefix}^{(k)}_m\right)
\]
其中 \(t_m\) 表示该前缀的最后 token 位置（实现上即 `states[layer][-1]`）。

对正/负集合取均值（论文 Eq. (contrastive)，实现 `stage1.py` 中 `pos_last.mean(0)` / `neg_last.mean(0)`）：
\[
\bar{h}^{(\ell)}_{P}(x,m) := \frac{1}{|\mathcal{P}|}\sum_{k\in\mathcal{P}} h_{m}^{(\ell)}(x, y^{(k)}),\qquad
\bar{h}^{(\ell)}_{N}(x,m) := \frac{1}{|\mathcal{N}|}\sum_{k\in\mathcal{N}} h_{m}^{(\ell)}(x, y^{(k)})
\]

定义 **raw correction vector**（不归一化）：
\[
v^{(\ell)}(x,m) := \bar{h}^{(\ell)}_{P}(x,m) - \bar{h}^{(\ell)}_{N}(x,m)
\]

### 3.6 v4.5.3 的“双条目存储”（wrong/right）

每个 \((x,m,\ell)\) 实际导出两条记忆条目（与 `README.md` 一致；实现见 `esm/offline/stage1.py` 的 export 部分）：

1. **wrong-entry（可注入）**：以“错误态 key”作为检索键，value 为 correction vector
   \[
   (\;k=\bar{h}^{(\ell)}_{N}(x,m),\;\; v=v^{(\ell)}(x,m)\;)
   \]
2. **right-entry（null-action）**：以“正确态 key”作为检索键，但 value 为空/零向量（在线会并入 null 分支）
   \[
   (\;k=\bar{h}^{(\ell)}_{P}(x,m),\;\; v=\mathbf{0}\;)\quad\text{（实现上 `vector_path=None`）}
   \]

**直觉**：当在线状态更像“正确轨迹”时，更容易检索到 right-entry，从而提高选择 null（不注入）的概率，抑制过度干预。

### 3.7 候选质量分（Quality / reward gap）

候选质量只用 reward gap，不依赖 \(\|v\|\)（论文 Eq. (Q)，代码 `reward_gap = mean(pos_rewards) - mean(neg_rewards)`）：
\[
Q(x,m) := \bar{R}_{P}(x,m) - \bar{R}_{N}(x,m)
\]
其中 \(\bar{R}_{P}\) 与 \(\bar{R}_{N}\) 分别是正/负集合上 \(R_{\eta_0}\) 的均值。

实现约束：
- 仅保留 \(Q(x,m)>0\) 且数值有限的候选（否则跳过）。
- 全局用一个 top-\(C\) heap（`offline_mine.keep_top_C`）保留质量最高的候选对（pair），再导出为文件。

### 3.8 Stage I 落盘内容

`outputs/<run>/mine/` 主要包含：

- `rollouts.jsonl`：每个样本的 rollouts（含 reward、correct、pred、text 等，用于调试/分析）。
- `candidates.jsonl`：候选条目清单（每个 pair 生成两条：wrong/right）。
- `keys/*.pt`：保存 key 向量（`*_wrong.pt` 与 `*_right.pt`）。
- `vectors/*.pt`：保存 delta 向量（`*_delta.pt`；仅 wrong-entry 使用）。

---

## 4. Stage II（select）：构建“高质量 + 多样”的工具库（DPP）

代码入口：`esm/offline/stage2.py:select_library`  
读取：`outputs/<run>/mine/candidates.jsonl`  
写入：`outputs/<run>/library/`

### 4.1 目标与关键点

Stage I 会产生大量候选条目（包含不同样本、不同控制点、不同层）。Stage II 要从中选择大小为 \(B\) 的子集 \(S\)（`offline_select.B`），同时：

- **质量高**：更偏好 \(Q_i\) 大的条目；
- **多样性强**：更偏好 key 隐藏态彼此“远”（避免全堆在同一模式上）。

注意：多样性只用 **key** 的相似度，而不看 delta 向量。

### 4.2 key 归一化与相似度核

对每个候选条目 \(i\) 的 key 向量 \(h_i\)，Stage II 会读入 `.pt` 并做 L2 归一化：
\[
\tilde{h}_i := \frac{h_i}{\|h_i\|_2}
\]

定义 Gram 相似度核（代码：`K = vectors @ vectors.T`）：
\[
K_{ij} := \tilde{h}_i^\top \tilde{h}_j
\]

### 4.3 质量 + 多样性的 DPP 目标（论文 Eq. (dpp_obj)）

Stage II 的默认方法为 `method=dpp`，优化如下目标：
\[
\max_{S:\,|S|=B}\;\; \sum_{i\in S} \log(1+Q_i)\;+\;\lambda \log\det\left(K_S + \epsilon I\right)
\]

- \(Q_i\)：Stage I 写入的 `quality`（即 reward gap）。
- \(\lambda\)：`offline_select.lambda_diversity`
- \(\epsilon\)：`offline_select.epsilon`（数值稳定）
- \(K_S\)：从 \(K\) 取出索引集合 \(S\) 对应的子矩阵。

实现用**贪心近似**（`esm/offline/stage2.py:greedy_quality_dpp`）：逐个加入能最大化“增益”的条目，并用 Cholesky 维护 \(\log\det\) 的边际增益。

### 4.4 额外约束：按控制点覆盖（可选）

`offline_select.min_per_control_point` 允许在全局选择前，先对每个控制点 \(m\) 强制保留若干个高质量条目（防止库在某些 \(m\) 上为空）。

### 4.5 Stage II 落盘内容

`outputs/<run>/library/`：

- `library.jsonl`：最终库清单（包含 `lib_id/tool_name/layer/control_point_m/quality/vector_path` 等元信息）。
- `keys/*`：复制出的 key `.pt`
- `vectors/*`：复制出的 delta `.pt`（仅对有 `vector_path` 的条目）

---

## 5. Stage III（memory）：构建在线检索索引（EpisodicMemory）

代码入口：`esm/offline/stage3.py:build_memory`  
读取：`outputs/<run>/library/library.jsonl`  
写入：`outputs/<run>/memory/`

### 5.1 写入的索引结构

Stage III 本质是把库“变成一个向量检索表”：

- `keys.npy`：形状 \((N,d)\) 的 float32 矩阵，每行是一个 key（默认会 L2 归一化，`offline_memory.normalize_keys=true`）：
  \[
  \hat{h}_i := \frac{h_i}{\|h_i\|_2}
  \]
- `entries.jsonl`：与 `keys.npy` 行对齐的元信息（tool_name、layer、control_point_m、quality、vector_path 等）。

在线加载见 `esm/online/memory.py:EpisodicMemory.load`，会再次做防御性归一化，确保 key 为单位向量。

---

## 6. 在线 ESM：检索 → 探测 → 选择 → 稀疏注入 → 分段生成

代码入口：`esm/online/esm.py:run_esm_dataset`  
产物目录：`outputs/<run>/eval/<out_tag>/`

### 6.0 在线 batch size 与 steer vector 分块（`online.batch_size_examples` / `max_steer_vectors`）

`esm/online/esm.py:run_esm_dataset` 支持两种运行方式：

- `online.batch_size_examples = 1`：逐样本循环（最稳；每个样本内部再做分段/检索/探测）。
- `online.batch_size_examples > 1`：按 segment step 做跨样本 batching（更快；同一控制点 \(m\) 上把多个样本一起检索/探测/提交）。

需要特别注意 vLLM-steer 的硬限制：同一个 `llm.generate()` 批次里，“带 `steer_vector_request` 的请求”最多只能有 `max_steer_vectors` 个（由 `LLM(..., max_steer_vectors=...)` 决定；本仓库在线初始化通常设为 `max(8, model.max_num_seqs)`）。因此在线实现会：

- hidden-states capture：按 `model.max_num_seqs` 分块抓取（`hs.get_all_hidden_states_generate(..., max_tokens=1)`）。
- probing / commit：凡是需要 `steer_vector_request` 的调用，都按 `max_steer_vectors` 做 chunk（避免超出 slot 而报错）；null 分支不受此限制。

吞吐/开销直觉：在每个控制点，探测阶段会额外生成约 `(#probe_examples) * (L+1) * probe_tokens` 个 token（再加一次 commit 段生成），因此把 `online.batch_size_examples` 拉得很大时，probe 的“峰值并行”与总开销都会显著上升。

### 6.1 Greedy baseline（对照）

`greedy` 方法直接用 `decode.max_new_tokens=T_max` 完成整段生成：
\[
y \sim p_\theta(\cdot \mid \text{prompt})
\]
并在最终文本上抽取答案 `pred`，与 gold 比较得到 accuracy（`esm/online/greedy.py`）。

### 6.2 ESM 的“匹配预算”原则（committed vs probe）

ESM 把 token 预算拆成两类：

- **committed tokens**：最终答案文本里真正保留的 token，总数严格受 `T_max` 限制（`committed_used <= T_max`）。
- **probe tokens（额外开销）**：为了比较候选工具而生成的短片段，不写入最终答案，但会计入 `budget_used` 与 `probe_tokens_used`。

因此 ESM 的总生成量通常为：
\[
|y_{\text{committed}}| \;+\; \sum_{\text{control points}} (L{+}1)\cdot T_{\text{probe}}
\]
其中 \(L=\) `online.L`，\(T_{\text{probe}}=\) `online.probe_tokens`。

### 6.3 每个控制点的检索（Top-k cosine）

在生成第 \(m\) 段之前（\(m\ge 2\)），令 `mem_m = m-1`，取当前前缀 \(\text{prefix}\) 的最后 token 隐藏态作为 query key：

对每个需要的层 \(\ell\)（来自 memory entries 的 layer 集合），计算：
\[
q^{(\ell)} := h^{(\ell)}_{\text{last}}(\text{prefix}),\qquad \hat{q}^{(\ell)} := \frac{q^{(\ell)}}{\|q^{(\ell)}\|_2}
\]

对 memory 中所有满足 `control_point_m == mem_m` 且 layer 匹配的条目 \(i\)，计算 cosine 相似度（实现是点积）：
\[
s_i := \cos\!\left(\hat{h}_i, \hat{q}^{(\ell_i)}\right) = \hat{h}_i^\top \hat{q}^{(\ell_i)}
\]

取 top-\(k\)（`online.k_retrieve`）作为候选邻域：
\[
\mathcal{N}_k(\hat{q}) := \mathrm{TopK}_i(s_i)
\]

**检索置信度门控**（实现于 `EpisodicMemory.topk_debug`）：

- 若 top-1 相似度 \(<\) `online.min_sim`，则视为“低相似度”，不使用记忆（直接走 null/greedy 分支）。
- 若该 control point 的条目数 \(<\) `online.min_entries`，也视为“信息不足”。

### 6.4 similarity-weighted quality：\(\widehat{A}_i\)

对检索到的条目 \(i\)，定义（论文、代码一致）：
\[
\widehat{A}_i := s_i \cdot Q_i
\]
其中 \(Q_i\) 是 Stage I 写入的 `quality`（reward gap）。

### 6.5 null 分支：right-entry 的合并

对检索结果中 `vector_path=None` 的条目（即 right-entry / null-like），ESM 不把它当“可注入工具”，而是把它对 null 动作的支持度合并到：
\[
\widehat{A}_{\text{null}} := \max_{i:\,v_i=\mathbf{0}} \widehat{A}_i
\]
实现见 `esm/online/esm.py`：`null_ahat = max(null_ahat, ahat)`。

### 6.6 候选集裁剪：Top-\(L\)

把有 `vector_path` 的条目作为“可注入候选”，按 \(\widehat{A}_i\) 排序取前 \(L\) 个（`online.L`）。

得到候选集：
\[
\mathcal{C} := \{\text{null}\} \cup \{i_1,\dots,i_L\}
\]

### 6.7 工具探测（probing）与 Score

对每个候选 \(i\in\{i_1,\dots,i_L\}\)：

- 用该候选工具以**单位强度**注入（probe 用 `scale=1.0`），从同一前缀生成固定长度 \(T_{\text{probe}}\) 的短 continuation（实现上同一批次生成 `L+1` 个：每个候选 + null）。
- 记录该 probe 输出的平均 token logprob（代码用 `avg_logprob = cumulative_logprob / tokens`）。

记：
- \(\logprob_i\)：候选 \(i\) 的 probe 平均 logprob
- \(\logprob_{\text{null}}\)：null probe 的平均 logprob

定义（论文 Eq. (accept_score)，与 `esm/config.py` 注释一致）：
\[
\mathrm{Score}_i := \beta\cdot \widehat{A}_i + \rho\cdot(\logprob_i - \logprob_{\text{null}})
\]
其中 \(\beta=\) `online.beta`，\(\rho=\) `online.rho`。

null 的 score 定义为：
\[
\mathrm{Score}_{\text{null}} := \beta\cdot \widehat{A}_{\text{null}}
\]

**无探测消融**：当 `online.variant=no_probing`（或 `probe_tokens<=0` 或候选为空）时，只用 \(\beta\widehat{A}_i\) 做选择。

### 6.8 选择规则与 null 阈值

选取得分最高的候选：
\[
i^* := \arg\max_{i\in\mathcal{C}} \mathrm{Score}_i
\]

并应用一个“强制 null”阈值（代码：`tau_null`）：

- 若 \(i^*\neq \text{null}\) 且 \(\mathrm{Score}_{i^*} < \tau_{\text{null}}\)，则改选 null。

此外实现里还有一个 tie-break：当最优候选与 null 几乎相等（数值上差 \(\le 10^{-12}\)）时偏向 null，减少不必要的干预。

### 6.9 注入强度（score-scaled injection）

当最终选择的是某个非 null 候选 \(i^*\) 时，ESM 用**分数缩放**得到最终注入强度：
\[
\alpha := k_{\text{scale}}\cdot \mathrm{Score}_{i^*}
\]
其中 \(k_{\text{scale}}=\) `online.k_scale`。

然后在该候选指定层 \(\ell_{i^*}\) 的控制点 token 上进行注入（回到第 2 节的算子）：
\[
h \leftarrow h + \alpha \, v_{i^*}
\]
实现中 `v_{i^*}` 是 Stage I 导出的 raw delta（未归一化），`normalize=False`。

### 6.10 分段提交（commit segment）

探测产生的 tokens 一律丢弃；真正写入最终答案的是“commit”这一次生成：

- 从当前前缀 `prefix`，用 `stop=delimiter` 生成下一段，并设置 `include_stop_str_in_output=True`（delimiter 会被拼回输出，因此也会计入 committed token 数）。
- 若生成未以 delimiter 结束（例如触发了长度上限或 EOS），视为“分段提前结束”，后续控制点不再触发（与 `README.md` 的分段规则一致）。
- 当最多生成完 \(M\) 段后，如果仍有 committed budget 且未提前结束，代码会再做一次 **tail** 生成（`stop=None`，不再分段/不再注入），把剩余 token 预算用完（实现见 `esm/online/esm.py` 里 `phase="tail"` 的 step 记录）。

### 6.11 其他在线门控（可选）

`online.min_tool_m / max_tool_m` 可以让 ESM 只在某个控制点范围内使用工具（其余控制点强制 null/greedy）。

---

## 7. 评测与产物

### 7.1 预测抽取与正确性判定

- `pred` 由 `esm/eval/extractors.py:extract_pred` 从文本抽取（例如 GSM8K 抽取最后一个 `#### <num>`，MATH 抽 `\\boxed{}` 并做 LaTeX 归一化）。
- `correct` 由 `esm/eval/metrics.py:is_correct` 判定：
  - 大多数任务是归一化后的 exact match；
  - MATH 类任务额外用 SymPy 做等价判定。

### 7.2 eval 写入的核心文件

`outputs/<run>/eval/<tag>/`：

- `per_example.jsonl`：每条样本的完整输出（含 `text/pred/gold/correct/tokens_used`；ESM 额外含 `steps` 与 `budget_used/probe_tokens_used`）。
- `summary.jsonl`：汇总（n, acc, T_max）。

`outputs/<run>/tables/`：

- `main_results_single.csv`：该数据集单一 budget 点的主表行（Greedy/ESM）。
- `ablation.csv`：`eval.ablations` 配置的消融结果（如 `no_memory`、`no_probing`）。
- `diagnostics_T*.csv/.json`、`compare_T*.csv/.json`：诊断与对比（`esm/analysis/*`）。

`outputs/<run>/cases/`：

- `cases_T*.md`：改进/退化案例（从 per_example.jsonl 里抽样）。

---

## 8. 配置字段与公式的逐项对照（快速索引）

下面给出“公式 ↔ 配置 ↔ 代码”的最常用映射：

- 控制点数 \(M\)：`control_points.M` → `esm/offline/stage1.py`, `esm/online/esm.py`
- delimiter \(D\)：`control_points.segment_delimiter` → `stop=delimiter` / `_delimiter_end_offsets`
- rollout 数 \(K\)：`offline_mine.K`
- Stage I examples batch：`offline_mine.batch_size_examples`
- 正/负个数 \(K_+,K_-\)：`offline_mine.K_pos / K_neg`
- 长度正则 \(\eta_0\)：`offline_mine.eta0`，reward：\(R_{\eta_0}=R-\eta_0|y|/T_{\max}\)
- top-\(C\) 候选池：`offline_mine.keep_top_C`
- 工具库大小 \(B\)：`offline_select.B`
- DPP \(\lambda,\epsilon\)：`offline_select.lambda_diversity / epsilon`
- memory key 是否归一化：`offline_memory.normalize_keys`
- 检索 top-\(k\)：`online.k_retrieve`
- Online examples batch：`online.batch_size_examples`
- probe 候选数 \(L\)：`online.L`
- probe 长度 \(T_{\text{probe}}\)：`online.probe_tokens`
- Score 超参 \(\beta,\rho\)：`online.beta / rho`，\(\mathrm{Score}_i=\beta\widehat{A}_i+\rho(\logprob_i-\logprob_{null})\)
- 注入强度 \(k_{\text{scale}}\)：`online.k_scale`，\(\alpha=k_{\text{scale}}\cdot\mathrm{Score}\)
- null 阈值 \(\tau_{\text{null}}\)：`online.tau_null`
- 相似度门控：`online.min_sim / min_entries`
- 评测预算 \(T_{\max}\)：`decode.max_new_tokens`
- vLLM 并行上限：`model.max_num_seqs`（也用于 hidden-states capture chunk；在线默认用它设置 `max_steer_vectors`）

---

## 9. 与论文草稿的对应关系（建议阅读顺序）

若你希望先从“方法定义”再回看实现，推荐按论文草稿阅读：

1. `idea4.5.2.tex` 第 2 节：控制点与注入算子（对应第 1–2 节）
2. 第 4 节：离线 Stage I–III（对应第 3–5 节）
3. 第 5 节：在线检索、探测与 Score（对应第 6 节）
4. Appendix：实现与复杂度（本仓库基本是其代码化落地）
